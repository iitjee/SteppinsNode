Congratulations, your application is very popular, and now you're facing a scalability challenge. What are some of the causes of scalability issues in relational databases?

The relational model itself requires a relational database engine to manage rights in a very special way. To assure consistency and atomicity, it must lock rows and tables and only allow one writer access at a time. Protecting referential integrity across several tables and rows increases the time the lock has to be in effect. Increased locking time means less rights or updates per second, means higher latency of transactions, means slower application. [LOCKING]

Scaling out and replicating data to other servers can make things worse. If relational engines try to enforce consistency and extend these locks across the network, lock times are then longer, transaction latency is higher. This will actually make applications even slower.


How do we solve these issues in relational databases? By de-normalizing tables, we can speed things up. When a lock is taken, it's taken on a single table and not several tables, so that can reduce latency. But then we abandon the relational goodness that caused us to use the relational database in the first place. We can also relax the consistency model, say allow for dirty reads, allow for reading data before it was fully committed. But then we might lose some of the durability or consistency guarantees of the engine.


How does Mongo approach these issues? MongoDB has no schema, no tables, no rows, no columns and tables, and certainly no relationships between tables.<b> In Mongo, you have single document write scope. A document lives in a collection, but updating documents occur one at a time. So if any locking needed to occur, it will be much simpler, there's no need to extend locks across collections, there are no relationships to enforce. Further, there is no schema to protect. </b>

So whereas you could have had two tables and when you wanted to update both, you needed to lock both tables. In MongoDB, there's no such requirement, so locking can be much faster. In replication scenarios, Mongo lets you choose the consistency you want. Mongo does not let you lock across several Mongo servers. A replication set in Mongo consists of a single server, which will accept all rights and several secondary servers, which will be replicated too, but there are no locks extended from the primary to the secondaries. Now you can choose the consistency model you want. Your application can send a command to update a document on the server and wait until that document has been replicated to all servers involved, or it can choose to only wait for the primary to persist that document, or it can choose that a majority of the servers will be written to before the write was acknowledged, or it can choose to hand over the document to primary and not even care whether it was persisted or not. It's up to you. Your application decides if it wants higher latency, but absolute consistency or relaxed consistency and some guarantee of durability. Not having to wait until a document was written eventually to all databases means the server returns control to the application faster and your application can be freed up to do other things faster. Mongo also offers a special collection called a capped collection. Capped collections have a fixed size and automatically overwrite older documents. Because it has fixed size, it does not need to spend any time allocating space for new documents. If you want to ingest many, many documents in a very high clip, you should consider using a capped collection in Mongo.

